---
layout: post
title: Space Data Overview
date: 2025-05-18
categories: [Personnal]
image: https://github.com/user-attachments/assets/d548796d-1f05-4e89-9b72-6fb4c76cdcb9
description: "Overview of space data, how to capture it, process it and value it."
---

I assisted to the CNES “Data from Space” training that provided a comprehensive overview of Earth observation and mission data from how to capture the data to how to store the datasets on earth passing by all the processing and more. I took a lot of written notes during this training and the following is a structured and enhanced report based on my notes.

## Satellite Orbits and Sensor Types

**Orbital Regimes:** Satellites are placed in different orbits depending on mission needs. The three primary orbit categories are **Low Earth Orbit (LEO)**, **Medium Earth Orbit (MEO)**, and **Geostationary Orbit (GEO)**. LEO typically ranges from \~200 km up to 2000 km altitude. Many Earth observation satellites operate in LEO, often in polar sun-synchronous orbits, which allow global coverage and consistent local solar lighting conditions on each pass (useful for imaging comparisons over time). At \~7.8 km/s, LEO satellites circle Earth in \~90 minutes, giving high revisit frequency. MEO spans \~5,000–20,000 km altitude and is commonly used by navigation constellations like GPS and Europe’s Galileo. **Geostationary orbit** lies at \~35,786 km altitude where satellites orbit in Earth’s equatorial plane with a 24-hour period, effectively “hovering” over a fixed ground spot. GEO is ideal for communications and continuous weather monitoring since one satellite can view a large portion of the hemisphere continuously. In practice, just three GEO satellites can provide near-global coverage at low latitudes. Each orbit type balances coverage, resolution, and revisit: LEO offers high spatial detail and global reach (with multiple satellites), while GEO offers constant vigilance over one area at coarser resolution due to distance.

**Active vs. Passive Sensors:** Satellite **sensors** generally fall into two categories: **active** and **passive**. Passive sensors detect natural energy (like sunlight reflected off Earth or thermal emission) without emitting their own signal. **Optical imagers** (e.g. standard RGB or multispectral cameras) are passive, relying on sunlight reflected from surfaces, so they only work in daylight (for visible bands) and are hindered by clouds. For instance, the Copernicus *Sentinel-2* satellites carry passive **multispectral** cameras that measure reflected sunlight in 13 bands to map land cover and vegetation health. **Thermal infrared** sensors (also passive) detect emitted heat and can map temperature differences – useful for **urban heat island** mapping or wildfire monitoring at night. In contrast, **active sensors** emit energy and measure the return signal. Examples include **radar** and **LiDAR** instruments. A radar satellite (an active microwave sensor) sends radio pulses to the surface and records the backscatter, enabling imaging regardless of daylight or weather. *Sentinel-1*, carrying a **Synthetic Aperture Radar (SAR)**, is a prime example of an active sensor mission. SAR technology sends out its own microwave pulses and captures the echoes to produce high-resolution images day or night, even through clouds. **LiDAR** (laser altimetry) instruments similarly emit laser pulses and measure the reflection, often used for high-precision elevation mapping (e.g. NASA’s ICESat for ice sheet altimetry). Active sensors are invaluable for penetrating cloud cover or darkness – for example, radar imaging can observe flooded areas during a storm through clouds, and LiDAR can map forest structure by probing between leaves.

**Common Orbit-Sensor Pairings:** Many Earth observation missions in LEO use sun-synchronous orbits to ensure consistent lighting for passive optical sensors – e.g., the French **Pléiades** and **SPOT** satellites and ESA’s *Sentinel-2* are in sun-synchronous LEO, passing overhead at the same local solar time each cycle to keep shadows and illumination constant. Active radar missions can also use polar orbits for global coverage (*Sentinel-1* SAR in LEO), whereas **weather satellites** often use GEO to constantly watch cloud movements (e.g., Meteosat, GOES). Navigation satellites (GPS, Galileo) are typically in MEO to cover wide areas with fewer satellites, and astrophysics missions may use specialized orbits (for instance, the **Gaia** space telescope orbits around the Sun-Earth L2 Lagrange point to have a stable deep-space environment for its observations). Each mission’s orbit is chosen carefully to maximize the effectiveness of its sensors and fulfill its objectives under the constraints of physics and coverage.

## Notable Earth Observation and Science Missions

Numerous **satellite missions** exemplify the variety of data collected from space. A few key missions covered in the training include:

* **Sentinel Series (Copernicus Program):** ESA’s Sentinel satellites provide a suite of Earth observations for environmental monitoring. For example, *Sentinel-1* is a radar mission for all-weather imaging (useful for ice monitoring, flood mapping, etc.), *Sentinel-2* is an optical mission capturing high-resolution multispectral images of land and coasts every \~5 days, *Sentinel-3* monitors oceans and land with instruments for sea surface height, sea surface temperature and ocean color, and *Sentinel-5P* measures atmospheric trace gases (air quality and climate gases). These satellites operate in constellation to ensure frequent revisit and robust data continuity.

* **SWOT (Surface Water and Ocean Topography):** A recent NASA–CNES mission (launched 2022) that uses advanced radar interferometry to measure water surface elevations in two dimensions. *SWOT* will survey **global ocean levels, lakes, and rivers** in unprecedented detail, crucial for climate studies and hydrology. It carries the Ka-band Radar Interferometer (KaRIn) instrument, an active sensor that sends radar pulses and uses two antennas to measure return signals and derive water heights across a 50 km wide swath. Notably, SWOT produces a **massive data volume** – roughly *1 terabyte of raw data per day* is downlinked, and after processing and calibration this can expand to \~20 TB of science data daily. This exemplifies the “big data” challenge of modern missions. SWOT’s data will support applications like tracking sea-level rise, monitoring reservoir levels, and river flood dynamics.

* **Gaia:** An ESA space observatory (with CNES involvement) launched in 2013, which has created the **largest 3D map of the Milky Way**. Gaia operates at the L2 point (\~1.5 million km from Earth) and uses astrometric measurements to pinpoint the positions and motions of nearly 2 **billion** stars with micro-arcsecond accuracy. Over its 11-year mission, Gaia revolutionized astronomy by measuring stellar distances, motions, and characteristics, enabling discoveries from new asteroids to insights into galactic evolution. Although not an Earth observation mission, Gaia underscores the diversity of “data from space” – from Earth climate to deep-space surveys – and the technical complexity of managing huge datasets (Gaia’s data releases involve petabytes of catalog information).

* **Other Missions:** The training also highlighted missions like **SMOS** (Soil Moisture and Ocean Salinity, a passive microwave radiometer for soil moisture), **COROT** (a French exoplanet telescope), **MicroCarb** (upcoming CNES mission to measure atmospheric CO₂), **IASI** on MetOp (infrared atmospheric sounder for weather and climate), **Trishna** (planned thermal infrared mission for Earth’s heat emissions), and **Mars 2020 / Perseverance rover** as an example of a planetary mission. Each mission showcases different sensor technologies and objectives. For instance, NASA’s **Mars Perseverance rover** carries instruments (cameras, spectrometers, a laser, etc.) and produces imagery and science data on Mars – but the data volume is relatively small (only on the order of **\~20 MB of science data per day** transmitted back) due to distance and bandwidth limits. The Mars rover relies on orbiters like Mars Reconnaissance Orbiter and ExoMars TGO to relay data to Earth, highlighting how **deep-space exploration** has its own data constraints and ground network needs (via NASA’s Deep Space Network, etc.). Despite low volumes, these planetary datasets are precious for research (e.g., mapping Mars geology or searching for past life) and require careful planning to downlink and archive.

## Applications of Space Data

One goal of the training is to show how spaceborne data translates into **real-world applications**. Satellites provide a unique vantage point and consistent, global measurements that support a wide range of use cases. Key application areas discussed include:

* **Climate and Environmental Monitoring:** Space data is indispensable for tracking climate change indicators. Satellites monitor **global temperature trends, greenhouse gas concentrations, and sea-level rise**. For example, altimetry missions have recorded a steady increase in mean sea level over decades. Earth observation also enables monitoring of ice caps and glaciers – radar and laser altimeters measure ice sheet thickness and extent, while optical imagers track the shrinking of Arctic sea ice coverage. Climate-focused missions (like the upcoming *MicroCarb* for CO₂ or ESA’s *Climate Change Initiative* datasets) provide consistent, long-term records needed for climate models and policy decisions. The *Space for Climate Observatory (SCO)* is an initiative leveraging such data for climate resilience planning.

* **Urban Planning and Heat Mapping:** High-resolution thermal infrared sensors produce “heat maps” of urban areas to identify **urban heat islands** – zones in cities that trap heat. By mapping land surface temperature at night, city planners can pinpoint hotspots and guide interventions (like adding green spaces or reflective materials). Only satellites with **thermal** imaging (e.g., Landsat’s thermal band or missions like ECOSTRESS, and the planned *Trishna*) can provide such city-scale temperature maps. Optical data is also used for urban expansion monitoring, infrastructure mapping, and even traffic pattern analysis (e.g., using night-time lights data).

* **Deforestation and Land Use:** Satellite time-series imagery allows **tracking of deforestation** and land cover change. Missions like *Sentinel-2* or *Landsat* provide frequent multispectral images to detect vegetation loss. For instance, illegal logging or **gold mining in remote forests** can be detected by comparing imagery over time – the training highlighted cases of **illegal gold mining (orpaillage)** in the Amazon and elsewhere, where sudden forest clearing is visible from space. Synthetic Aperture Radar (Sentinel-1) can also see forest structure changes and is useful in tropical areas with frequent clouds. These data feed into deforestation alerts and conservation efforts. Similarly, satellites help monitor crop health and predict agricultural yield via vegetation indices.

* **Illegal Activity Detection:** Beyond environmental crimes, satellites assist in detecting various illicit activities. Examples include **illegal fishing** (using satellite AIS data and radar to find vessels in restricted areas), **illegal mining or quarrying** (observing landscape scars), and **illegal waste dumping**. The training specifically mentioned **illegal dump detection** – high-resolution images can reveal unauthorized landfill sites or trash dumps. Early detection is key to prevent such dumps from growing; analysis might include proximity to sensitive areas like water sources or communities to assess the risk. Multispectral data can even identify chemical signatures of certain wastes or disturbances in vegetation that hint at pollution. This application shows how remote sensing intersects with regulatory enforcement and public health.

* **Disaster Response and Environmental Hazards:** When natural disasters strike, satellites are often the first to provide situational awareness. The **International Charter on Space and Major Disasters** is an agreement through which space agencies provide free satellite imagery to emergency responders. Use cases include mapping flooded areas after hurricanes (radar can penetrate clouds to see flood extent), assessing wildfire burn scars, monitoring oil spills, or earthquake damage mapping via radar interferometry (InSAR). The training underlined how quickly satellite images are made available to aid agencies in crises – for example, imagery was provided after the Beirut explosion to map damage. Additionally, satellites continuously watch volcanic ash plumes (important for aviation), track severe storms, and monitor droughts or crop failures, aiding early warning and disaster preparedness.

* **Sea Level and Coastal Change:** Decades of **radar altimetry** missions (TOPEX/Poseidon, Jason series, Sentinel-6) have measured sea surface height with centimeter accuracy. These data reveal not only global sea-level rise but also regional variations (as ocean currents and temperatures cause uneven distribution of sea level). By comparing historical altimeter records, scientists can better attribute sources of sea rise (e.g., thermal expansion vs. melting ice). Satellites also track coastal erosion and shoreline changes using optical imagery – a field called **coastal digital analysis**. For example, comparing images of a shoreline over years can quantify beach erosion or delta land loss. This information is critical for adapting infrastructure and managing coastal ecosystems.

* **Mars and Planetary Exploration:** Space data applications go beyond Earth. The training touched on how satellite data supports **planetary exploration**, particularly Mars. Orbital imagers (like Mars Reconnaissance Orbiter’s HiRISE camera) have mapped Mars at sub-meter resolution, helping select rover landing sites and guiding rovers to interesting targets. The **ground segment** for Mars missions must plan data relay carefully, since rovers like Perseverance generate relatively small data volumes (a few tens of MB/day) due to limited bandwidth. Even so, every image and spectrometer reading from Mars is invaluable for science – e.g., detecting signs of past water or analyzing rock compositions. The combination of orbiter and rover data provides a multi-scale understanding of Mars. Techniques originally developed for Earth observation (such as image mosaicking, digital elevation model extraction from stereo images, and spectral analysis) are extensively applied to Mars imagery as well. Thus, “data from space” includes interplanetary datasets, and the principles of remote sensing and data management apply similarly.

## Ground Segment Operations and Mission Planning

Operating satellites and delivering their data involves a complex **ground segment** – the set of ground-based facilities and processes that communicate with spacecraft, task their instruments, and process the incoming data. The training emphasized how crucial this segment is in the space data value chain:

* **Satellite Tasking and Mission Planning:** For Earth observation missions, especially high-resolution imaging satellites, **tasking** refers to scheduling the satellite’s data acquisitions (e.g. which area to image at what time). Satellites have limited imaging capacity per orbit and onboard storage, so ground teams (or sometimes automated planning systems) must prioritize requests. This involves mission planning software that takes into account the satellite’s orbit, agility (slew constraints), lighting conditions, and cloud forecasts to optimize image captures. For example, commercial imagery programs (like Airbus’s Pléiades or the **Dinamis** program) allow users to request imagery; the ground segment then computes when the satellite can point and shoot that target. **Mission planning** also covers long-term orbit maintenance (maneuvers to counteract drag for LEO or maintain GEO position) and scheduling any special operations (like calibrations or downlinks). In the CNES training, participants learned how various mission phases (from **Phase 0/A feasibility** through design, construction, launch, and operations) involve evolving planning needs.

* **Ground Stations and Communications:** Once a satellite collects data, it must transmit it to Earth. Ground stations with large antennas receive these downlinks. Different frequency bands are used: **S-band** is often for telemetry and telecommand (sending instructions to the satellite and basic health data down), whereas **X-band** (higher frequency) is commonly used for downlinking the bulk of science data. The training notes the use of a network of ground stations (for example, a high-latitude station like Kiruna in Sweden is used to receive data from polar-orbiting satellites on each pass). These stations forward the data via high-speed networks to the main processing center (e.g., CNES’s center in Toulouse). Some systems include **local processing at station** (edge computing buffers) to quickly check data quality. Mission operations teams also manage communication windows – a LEO satellite might only be in range of a given station for \~10 minutes per orbit, so high downlink rates (hundreds of Mb/s) and efficient scheduling are needed to dump all stored data. For **deep-space missions** (like Mars rovers or Gaia at L2), communication is even more constrained – they rely on limited **Deep Space Network** passes or relay orbiters, and long signal travel times (e.g. \~15 minutes one-way to Mars) mean careful planning of command uplinks and downlink windows.

* **Satellite Control and Health Operations:** In addition to data downlink, the ground segment continuously monitors the satellite’s health and status. Telemetry data on power, temperature, orbit, etc., are analyzed by engineers. **Ground control centers** (such as CNES’s control center in Toulouse or ESA’s ESOC) send up commands for tasks like adjusting orientation, updating onboard software, or initiating instrument calibrations. The training likely covered how ground controllers use software to automate routine operations and only intervene for anomalies. Modern missions also employ autonomous onboard scheduling and fault management to reduce ground load, but the ground segment remains the ultimate supervisor of the mission.

* **Data Reception and Initial Processing:** When raw data is received on the ground, it typically arrives as **Level-0** data – essentially a stream of instrument measurements (plus headers, timestamps, etc.). The ground segment’s processing pipelines then convert this to usable products through successive levels (discussed in the next section). **Real-time processing** can generate quick-look images or near-immediate hazard warnings (e.g., a quick flood map for disaster response) within hours, while more refined processing is done in batch mode. The CNES center, for example, automatically processes data from Sentinel and other missions continuously, leveraging its compute infrastructure. The notes mention **orbital calculations** and other housekeeping computations also done at the processing center.

* **Ground Segment for Science Missions:** For missions like Gaia or Mars rovers, the ground segment may be highly specialized. The training highlighted that for **science/astronomy missions** the ground segment design depends heavily on mission specifics. For instance, Gaia’s data processing involves a consortium that runs **dedicated pipelines to calibrate stellar positions**, requiring a network of data centers across Europe. Mars missions must integrate NASA’s DSN or ESA’s Estrack for comms. A point from the training: the Mars 2020 rover’s ground segment had to manage very low data volumes (\~20 MB/day) but high scientific value, using relay orbiters and a deep-space communication schedule. Thus, “ground segment operations” ranges from orchestrating massive Earth observation data flows to carefully handling trickles of deep-space data – all requiring rigorous planning, reliability, and sometimes international cooperation.

## Data Levels and Product Quality

Satellite data undergoes a series of processing **levels** before it’s ready for end-users. The training introduced the standard nomenclature for **data product levels 0 to 4**, as defined by space agencies like NASA/ESA:

* **Level 0:** Raw data at full instrument resolution, as received from the satellite, with communication artefacts removed. This is essentially the unprocessed digital signal (e.g., raw sensor counts or waveform packets). Level-0 is rarely distributed to users except perhaps instrument teams, because it’s not yet calibrated or easily interpretable.

* **Level 1:** Reconstructed sensor data that has been time-referenced and annotated with calibration information and geometric parameters. For example, **Level 1A** might include raw pixel data with timing for each line and sensor telemetry. **Level 1B** typically means the data has been converted to physical sensor units (e.g., radiances or backscatter coefficients) using radiometric calibration coefficients. For optical images, Level-1 often implies the image is still in “sensor geometry” (not map-projected) but radiometrically corrected. Some missions have **Level 1C** which is further processed – for instance, Sentinel-2 Level-1C is Top-of-Atmosphere reflectance in a UTM map projection (orthorectified using a DEM).

* **Level 2:** Data that has been **geophysically corrected** and derived from Level 1. These are the essential geophysical variables at the same location and resolution as the original measurements. Examples: satellite imagery corrected for atmospheric effects to yield **surface reflectance** (for optical images) is a Level-2 product. Sea surface height or significant wave height computed from altimeter echoes is a Level-2 product. Atmospheric sounding profiles (temperature, humidity vs. altitude) derived from a spectrometer are Level-2. Essentially, Level-2 is “retrievals” of the quantity of interest from the direct measurements.

* **Level 3:** Data that has been **gridded, merged, or otherwise resampled in space or time**, often combining multiple orbits or instruments. Level-3 products are typically on regular grids and may be averaged or composited over time. For instance, a **monthly global sea surface temperature map** at 0.1° resolution, compiled from many satellite passes, would be Level-3. A 10-day cloud-free mosaic of NDVI (vegetation index) over a country from daily images is Level-3. These products sacrifice some original resolution or timing in exchange for completeness and ease of use in analysis.

* **Level 4:** **Model outputs or analysis results** derived from lower-level data. These involve assimilation or combination of data with models. For example, a climate reanalysis that assimilates satellite observations into a weather model to estimate atmospheric states is a Level-4 product. Another example is a biomass estimate map produced by combining optical and radar data (Level-2) with ground data through a model – the output biomass map is Level-4. Level-4 often provides the *information* most end-users ultimately want (e.g., crop yield predictions, or ocean currents), at the cost of being furthest from the raw measurements.

The training stressed understanding these levels as they relate to data **latency and quality**: e.g., Level-1 and 2 are produced soon after downlink (within hours) to serve users needing quick data, whereas Level-3/4 products might be delivered later after more processing.

**Image Quality – Radiometric and Geometric:** Two key aspects of data quality are **radiometric fidelity** and **geometric accuracy**.

* *Radiometric Quality:* refers to the correctness and precision of the pixel values (intensities) in the data. Radiometric calibration ensures that the sensor’s raw digital numbers are converted to physical units like radiance or reflectance, accounting for sensor sensitivity, solar illumination, and other factors. Good radiometric quality means if two images show a region’s reflectance as 0.25 (25%), they truly correspond to the same brightness, enabling quantitative comparisons. Radiometric quality also involves high **radiometric resolution** (enough bits per pixel to discern small differences – e.g., 12-bit or 16-bit data can distinguish finer intensity gradations than 8-bit). Additionally, radiometric calibration often uses known reference targets (like onboard calibration lamps or ground test sites) to adjust any drift in sensor response. For example, Sentinel-2 uses calibration sites in deserts to validate that its measured reflectance is accurate to within a few percent. A related concept is **signal-to-noise ratio (SNR)** – higher SNR means the sensor can detect subtle signals above instrument noise. The training highlighted that factors like sun angle, atmospheric state, and sensor settings can affect radiometry, hence higher-level products often include corrections for those (like Top-of-Canopy reflectance accounting for atmospheric absorption). *Radiometric quality* is crucial for applications such as climate trends (where biases of 0.1 K in temperature can matter) and vegetation change detection.

* *Geometric Quality:* refers to the spatial accuracy of the data – ensuring that features in the image or dataset align with their true positions on Earth (or on a map projection). Raw images from satellites may have geometric distortions due to sensor view angle, Earth’s curvature, and terrain relief. For instance, a satellite imaging at an angle will see tall buildings or mountains displaced from their base location (the “lean” in imagery). The process of **orthorectification** corrects these distortions by using sensor geometry and a digital elevation model to map each pixel to the proper geographic coordinate. After orthorectification, an image becomes a map-accurate “orthoimage” where distances and areas can be measured reliably. Good geometric quality also means accurately knowing the satellite’s position and attitude – any errors there can shift the image on the ground. Modern missions use GPS, star trackers, and gyros to pinpoint the sensor orientation, often achieving sub-pixel geolocation accuracy after processing. The training likely discussed **ground control points** (known landmarks) used to refine geoaccuracy. **Geometry** also includes internal distortions like lens or detector effects, which are calibrated pre-launch and corrected in processing. In summary, high geometric quality ensures that multi-date images overlay correctly (vital for change detection) and that the data can be integrated into GIS with other spatial data. Orthorectification and co-registration of images are standard steps to achieve this quality.

By delivering data at consistent processing levels with known radiometric and geometric quality, space agencies ensure that end users (analysts, scientists, downstream services) can trust and easily use the data without being remote sensing experts themselves. The CNES training underscored how much effort in the ground segment goes into calibration, validation, and quality control for each mission’s data products.

## Resolution, Calibration, and Data Formats

Different satellite instruments have varying **resolutions** and require careful calibration. Key concepts covered include:

* **Spatial and Spectral Resolution:** Spatial resolution is the ground size of one pixel in an image (the detail level). High-resolution optical satellites like Pléiades Neo or WorldView-3 have \~30 cm pixels, resolving individual buildings. In contrast, a satellite like MODIS has \~500 m pixels for global daily coverage. There is often a trade-off: higher spatial resolution means a smaller swath or less frequent coverage due to data volume limits. **Spectral resolution** refers to the number and width of wavelength bands an instrument measures. Multispectral sensors (e.g., Sentinel-2) have around 10–15 broad bands (blue, green, red, NIR, SWIR, etc.), whereas **hyperspectral** sensors may have hundreds of narrow contiguous bands (like the upcoming EnMAP or Prism missions). High spectral resolution allows detection of specific materials (e.g., identifying tree species or minerals by their spectral signature) but also generates larger data volumes. A well-known trade-off is that improving spatial resolution often forces wider spectral bands or fewer bands (due to sensor design limits) – hence missions like Pleiades have a few broad bands at very high resolution, while missions like Hyperion had many bands at coarser resolution. The training discussed how **pan-sharpening** techniques merge high spatial resolution panchromatic images with lower-res multispectral images to yield a high-res color image. For example, Pleiades provides a 0.5 m panchromatic band and 2 m multispectral bands; by pan-sharpening, one can get a 0.5 m color image. This process requires precise registration of images and careful algorithms to avoid spectral distortion, as described in CNES’s Orfeo ToolBox documentation. The result combines the “sharpness” of the panchromatic with the color detail of multispectral.

* **Radiometric Calibration and Atmospheric Correction:** As introduced earlier, radiometric calibration converts raw sensor readings to physical units. This involves applying calibration coefficients (from pre-launch lab measurements or in-flight references) to account for instrument gain, detector sensitivities, and drift over time. Many modern satellites have onboard calibration devices – e.g., Sentinel-2 has a diffuser panel that reflects sunlight into the sensor to check its calibration. Calibration also accounts for the solar irradiance (especially for optical sensors) to express values in reflectance or radiance standardized for sun-earth distance and angle. **Atmospheric correction** is a further step to remove the influence of the atmosphere on measured signals. For optical imaging, sunlight is scattered and absorbed as it travels through the atmosphere twice (sun to ground, and ground to satellite). Gases like ozone, and aerosols like dust, particularly affect shorter wavelengths (blue light). Algorithms such as NASA’s DOS (Dark Object Subtraction) or more sophisticated physics-based models (using radiative transfer codes) estimate the amount of scattering and absorption, often by analyzing the image itself (e.g., finding dark pixels to infer haze) or using external data like aerosol optical depth from other instruments. After atmospheric correction, **Top-of-Canopy (surface) reflectance** is obtained, which is what you’d measure just above the ground if no atmosphere was in between. This is crucial for quantitative analyses (e.g., comparing vegetation indices across dates or locations). Similarly, in **radar data**, signals can be affected by atmospheric water vapor delaying the radar pulses (especially for high-frequency radars), which is corrected using models or calibration targets. In **atmospheric sounding** missions, calibration is about tying measurements to known gas absorption spectra and accounting for instrument spectral response.

* **Atmospheric Sounding:** A special case of remote sensing, atmospheric sounding involves measuring the vertical profiles of atmospheric properties (temperature, humidity, gas concentrations) by satellites. Instruments like infrared sounders (e.g., IASI, AIRS) observe the upwelling thermal radiation in many spectral channels; each channel is sensitive to a certain altitude range due to absorption lines. By solving an **inverse problem**, scientists retrieve how temperature and moisture vary with height. Another method, **radio occultation** (GNSS-RO), uses GPS signals bending through the atmosphere to derive density and temperature profiles. The training likely touched on how sounding data feeds weather models – indeed, satellites now provide hundreds of thousands of atmospheric **soundings** globally per day, greatly improving weather forecast skill. Sounding data products are typically Level-2 profiles with associated error estimates. Calibration for sounders ensures that measured radiances match known reference spectra (for instance, using well-characterized blackbody sources in orbit to calibrate an IR sounder’s radiance measurements).

* **Data Product Formats:** Space data comes in various formats optimized for different data types. Common **raster image formats** include GeoTIFF (often used for processed optical imagery) and **JPEG2000** (used, for example, in Sentinel-2 Level-1C products as 100 km² tile files) which provides compression. Specialized scientific data often uses **NetCDF** or **HDF5** files – these can store multi-dimensional arrays (e.g., 3D data cubes with latitude, longitude, time, plus multiple variables) along with rich metadata. NetCDF is widely used for gridded products like sea surface temperature, atmospheric model outputs, etc., because it supports self-description and is standard in climate science. The training also discussed emerging **cloud-optimized formats**: for instance, **Cloud Optimized GeoTIFF (COG)** is a GeoTIFF structured internally so that parts of the image can be accessed via HTTP range requests (allowing you to fetch just a tile or a specific resolution). This is useful for web map services and cloud processing, as you don’t need to download the entire file to view or process a subset. **Zarr** is another modern format for large N-dimensional data, designed for chunked storage and parallel access. Unlike monolithic NetCDF files, Zarr stores data in small compressed chunks (often as directory of binaries or in object storage) which can be read/written independently, enabling distributed computing. The training provided a striking comparison: reading a SWOT dataset in traditional NetCDF format serially took \~90 minutes, whereas using a parallelized **Zarr** approach cut it to \~2 minutes. This orders-of-magnitude speedup in data access is crucial for “big data” scalability. For vector data (points, lines, polygons like GIS layers), formats like **Shapefile**, GeoJSON, or Spatio-temporal databases are used; however, most Earth observation data by volume are raster/gridded.

* **Data Catalogs and Metadata:** Along with formats, the training highlighted the importance of data **cataloguing** and standardized metadata. With petabytes of imagery available, efficient search and retrieval is key. Standards like **STAC (SpatioTemporal Asset Catalog)** define a common JSON-based metadata structure to describe geospatial assets for easy indexing. CNES developed tools like **REGARDS** (a generic catalog system) and uses **OpenSearch** APIs to allow users to query satellite data holdings. Every data product is typically accompanied by metadata such as acquisition time, geolocation (footprint, projection), sensor info, cloud cover percentage (for optical), etc. The use of **DOIs (Digital Object Identifiers)** for datasets is increasing, to facilitate citation and tracking of data usage.

In summary, this section of the training underscored that having cutting-edge sensors and satellites is only half the battle – ensuring their data is accurate (calibrated), accessible (in efficient formats with rich metadata), and appropriately tailored in resolution to the application is equally important.

## Remote Sensing Techniques and Processing

The course delved into several specialized techniques that enable deeper analysis of space data:

* **Synthetic Aperture Radar (SAR) Imaging:** SAR was discussed as a powerful active remote sensing technique. By moving a radar antenna along the orbital path and processing the reflected signals, SAR synthesizes a large antenna aperture, achieving high resolution imagery even from 700 km away. SAR can image the Earth in all weather and darkness, making it indispensable for applications like disaster response during cloudy conditions, ocean oil spill detection, or monitoring polar ice. The training likely explained how SAR data is different from optical – it measures the **backscatter coefficient** of the surface, which depends on surface roughness and moisture. This allows detecting, for example, differences between smooth water (which reflects radar away, appearing dark) and rough land or vegetation (bright). SAR images often look monochrome and speckled (due to coherent interference noise, called speckle). Participants learned about **polarimetry** (some SARs send/receive polarized waves to get additional information on target properties) and **InSAR (Interferometric SAR)**, where two SAR images (from slightly different positions or times) are combined to detect minute ground movements or elevation differences. InSAR is how we measure ground deformation from earthquakes or volcanoes with millimeter precision, using satellites like Sentinel-1. SAR processing from raw Level-0 data to images involves complex algorithms (Fourier transforms, motion compensation, etc.), but users usually work with Level-1 SAR products that are already focused images, in either slant range or projected to ground range. The course emphasized that SAR imagery provides complementary information to optical data and has wide applications in mapping, surveillance, and science.

* **Radar Altimetry:** A specific radar technique focused on measuring distances. **Radar altimeters**, unlike imaging SAR, typically point straight down (nadir-looking) and send very short pulses. By precisely timing the round-trip of the pulse and knowing the satellite’s exact orbit altitude, the distance to the sea surface is determined. After correcting for delays (atmospheric effects) and tides, one gets **sea surface height** relative to Earth’s center. The training covered how altimetry missions have revolutionized oceanography: from the first coarse-resolution altimeters to the latest interferometric ones like SWOT which will map water heights in 2D. Altimeters not only measure global mean sea level rise (which is about 3.3 mm/yr currently) but also map the shape of the ocean surface, which relates to currents and ocean heat content. The **precision orbit determination** is crucial – using laser ranging, GPS, and ground tracking, we know the satellite’s orbit to within a few cm, making the derived sea level measurement accurate to \~2 cm. Altimeters have also been used on land to measure large lakes, rivers, and even in mapping continental elevation (though their footprint is several kilometers, limiting land use). Future developments like SWOT’s Ka-band interferometric altimetry aim to resolve water bodies and ocean eddies with \~50m resolution, producing enormous data as noted before. The ground processing for altimetry involves signal waveform processing (to estimate the return pulse shape which tells wave height and wind speed) and applying geophysical corrections (tides, atmospheric pressure, ionospheric delay, etc.). The net result is a rich set of marine and hydrological data products enabling, for example, climate researchers to close the sea-level rise budget or humanitarian agencies to anticipate flood severity by observing river heights from space.

* **Optical Photogrammetry and 3D Reconstruction:** Photogrammetry is the art and science of making measurements from photographs – and with satellites, **stereo imaging** enables 3D model construction. The training likely introduced how missions like **SPOT 6/7 or Pleiades** capture stereo pairs (two images of the same area from different angles, either by two satellites or one satellite taking images from different orbit positions). By finding matching points in the image pair, one can triangulate their 3D coordinates, producing a **Digital Elevation Model (DEM)**. CNES’s upcoming **CO3D** mission is dedicated to producing high-resolution global DEMs using a constellation of stereo imaging microsatellites. Techniques of bundle block adjustment and rigorous sensor modeling come into play to get accurate 3D data. These DEMs have many uses: mapping terrain for geology, estimating volumes (e.g., mining operations), simulating flood zones, or even creating 3D city models for urban planning. The training highlighted that even for disaster response, 3D information is valuable – for instance, comparing DSMs (Digital Surface Models) before and after an earthquake can show building collapses or landslide volumes (this was touched on in a **“multimodal 2D/3D change detection”** exercise).

* **Orthorectification:** Already discussed under geometric quality, orthorectification is a foundational processing step. The training gave insight into how raw images are tied to Earth’s terrain. Participants likely experimented with orthorectifying an image using known sensor models and a DEM (possibly using tools like Orfeo ToolBox or commercial software). The effect of orthorectification is to remove perspective distortions: an image taken over mountainous terrain will, when raw, have mountain tops displaced; once orthorectified, the mountains sit exactly over their base locations on a map. This process involves computing a transformation grid using the satellite’s ephemeris (position and orientation), the camera model (internal optics), and the DEM to account for elevation changes. The image is then resampled to a map projection (like UTM). The training notes mention that certain products are labeled “ortho-ready” – meaning they include some approximate georeferencing to allow quick viewing, but still require true orthorectification for precision. The takeaway is that raw high-resolution images (Level-1A or 1B) should be orthorectified (Level-1C/2) before analysis to avoid spatial errors, especially in uneven terrain.

* **Pan-Sharpening:** As described, pan-sharpening is a frequent post-processing step for high-res optical imagery. Trainees learned why pan-sharpening is needed: sensors with a **panchromatic band** (typically covering a wide spectral range, thus high light throughput) can have finer spatial resolution than the multispectral bands. For example, WorldView-3’s panchromatic is 0.3 m while its multispectral bands are 1.2 m. By fusing these, one can create a 0.3 m color image. The process involves upsampling the multispectral images to the pan resolution, aligning them, and then merging – often by injecting high-frequency detail from the pan into the multispectral data. There are various algorithms (intensity-hue-saturation method, Brovey transform, wavelet-based fusion, etc.). The Orfeo ToolBox reference provided a simple formula and notes that pan-sharpening can be resource-intensive. The end result is visually sharp images that are useful for interpretation, although one must be cautious using them for quantitative analysis (since the spectral values are altered by the process). The training likely cautioned about appropriate uses of pan-sharpened images (e.g., great for map making or object detection, but if doing scientific analysis on pixel values like calculating NDVI, better to use original multispectral resolution to maintain spectral fidelity).

* **Data Fusion and Analytics:** In addition to these classical techniques, the training might have touched on newer analytical methods. For example, combining data from multiple sensors (optical + SAR synergy to detect changes or get all-weather coverage), or using **machine learning** on satellite data (the mention of “ML Ops?” in notes suggests discussion on applying AI to large EO datasets). Real-world examples include using convolutional neural networks on multi-sensor data to detect features like illegal mining or to map urban change. With cloud platforms and HPC, one can train models on terabytes of imagery. The importance of **DevOps and MLOps** was emphasized – meaning that deploying operational analytics on satellite data requires robust software practices (versioning models, continuous integration for code, containerization for portability, etc.). While not a traditional remote sensing “technique”, this is a growing aspect of space data exploitation.

Overall, the training’s coverage of these techniques underlines that extracting useful information from raw satellite measurements often requires sophisticated processing – whether it’s geometrically correcting images, deriving 3D models, fusing different data sources, or automatically detecting patterns. For a portfolio, familiarity with these methods demonstrates technical depth in handling and analyzing spaceborne data.

## Data Infrastructure and Big Data Handling

One of the most critical aspects of modern space data is managing the **sheer volume** and enabling timely processing. CNES and other agencies have built extensive infrastructure to handle “big data from space,” and the training provided insight into these systems:

* **High-Performance Computing (HPC) and High-Throughput Computing (HTC):** The CNES data center must support both **HPC** – running large parallel computations (e.g., modeling and simulations) – and **HTC** – processing many independent tasks at high volume (e.g., systematically processing thousands of satellite scenes daily). At CNES, much of the satellite data pipeline is an HTC scenario (deterministic processing of each granule), which is why HTC is heavily used. HPC comes into play for tasks like climate modeling or complex physical simulations that might accompany satellite projects. The training explained that CNES’s main computing facility (the **Data Processing Center, DPC**) nicknamed **T-REX**, contains on the order of *16,000 CPU cores* and *11–12 petabytes of fast storage* for processing. Additionally, CNES has a **data lake** of much larger capacity (dozens of petabytes on disk and tape) to archive mission data. Key numbers: CNES’s data lake offers **35 PB of disk and 35 PB of tape** storage to accommodate the ever-growing archives. The HPC cluster provides \~120 TB of RAM and even GPUs (CNES has \~60 GPUs for AI workloads). With SWOT and other missions coming online, the need for such capacity is clear. For instance, SWOT was estimated to generate **\~33,500 data files per day**, requiring \~3,100 CPU cores running almost continuously to process that daily influx. Traditional infrastructure might crumble under such loads, so HPC/HTC and parallelization are essential.

* **Resource Orchestration:** To efficiently use the computing resources, CNES employs batch schedulers and orchestration frameworks. The training notes mention **Slurm** (a widely used HPC job scheduler) for managing HPC jobs, and **Kubernetes** for container orchestration, likely in the context of cloud or on-prem cloud-like operations. Essentially, **jobs** – like processing a satellite pass or generating a composite – are queued and distributed across the cluster. The orchestration ensures **decomposition & parallelization** of tasks, scheduling tasks to available CPUs/GPUs, and managing dependencies. The goal is to **maximize resource utilization** while handling the big data flow. In practice, CNES might break a large data processing job (say reprocessing an entire archive) into many small tasks (per orbit, per tile) that run in parallel. The training indicated that moving from local computing to distributed HPC/Cloud introduces complexity at each step (laptop < server < cloud/HPC < multi-datacenter), so specialized knowledge and tools are needed. CNES does hybrid cloud bursts too – the data center can **scale up with cloud resources** for peak demands, integrating external cloud VMs or containers when internal resources are tight. Notably, containerization is used but with care: Docker wasn’t suitable on their HPC (due to security and network constraints), so they use **Singularity** which allows user-space containers without root privileges. This ensures researchers can bring their custom environments (software, libraries) to the HPC and run reproducible workflows without compromising the system.

* **Data Access and Platforms:** CNES participates in France’s national Research Infrastructure called **“IR Data Terra”** which is organized into thematic data hubs. The training highlighted four main poles: **Theia** (for continental surfaces, land data), **Aeris** (atmospheric data), **Odatis** (oceans), and **ForM\@Ter** (solid Earth and geodesy). Each acts as a portal providing access to specific domain datasets, tools, and support. For example, **ODATIS** is the oceanography hub, providing services like long-term ocean data archives, computing facilities to analyze ocean data, and specialized portals (like AVISO+ for altimetry data). ODATIS connects multiple data centers and offers not just raw data but also visualization tools (e.g., Ocean Virtual Laboratory), and even a **Virtual Research Environment** with Jupyter notebooks close to the data. The training described services ODATIS provides: HPC access (leveraging CNES’s HPC), large storage, data subsetting tools, and support for users (workshops, helpdesk). Likewise, **Theia** serves land users with products like snow cover maps, soil moisture, or high-resolution land cover, often derived from Sentinel data. The training pointed out that these platforms supersede older systems like the PEPS platform, by being more specialized and integrated. **Dinamis** and **GeoDés** were mentioned as well: Dinamis is a program to facilitate access to high-resolution commercial imagery for the French research community, and GeoDés is a catalog for certain CNES missions and Copernicus data. All these platforms aim to lower the barrier for scientists and value-added service providers to exploit satellite data – providing not just files, but also **APIs, cloud processing, and domain-specific tools**.

* **Long-Term Data Preservation:** Given that space missions often produce data continuously over years (and climate studies demand decades of data), archiving is a major concern. CNES’s data lake with a **“disk + tape” two-tier storage** is designed for this. **Disk storage** (online, synchronous) holds recent or frequently accessed data, while **tape archives** (often called “Glacier”, analogous to AWS Glacier) provide massive capacity with minimal energy use. Tapes are slower to access (hours to retrieve) but are very durable and cost-effective for infrequently accessed historical data. The “glacier” tape system at CNES uses no electricity when idle, and can reliably store petabytes for decades. The training emphasized the importance of **LTA (Long Term Archive)** for maintaining long time series – one cannot predict all future uses of the data, but by preserving raw and intermediate data (along with algorithms), we enable reprocessing and new analyses by future generations. There was also a reference to “the heritage of CERN” – CERN pioneered large-scale data grid and distributed storage to handle LHC data. Space agencies have adopted similar ideas (e.g., leveraging national research networks, distributed archives). For example, the **Gaia data network** connects multiple data centers to share the load of processing and distributing the enormous star catalog, reminiscent of how particle physics distributes computation.

* **Database and Search Technologies:** Managing metadata for millions of satellite images or billions of data points requires robust databases. The training notes mention use of **PostgreSQL/PostGIS** for geospatial relational data, **Elasticsearch** for indexing and search (likely powering fast queries for specific locations, dates, or free-text across metadata), and **NoSQL databases like MongoDB and time-series DBs like TimescaleDB**. Each has a role: PostGIS can handle vector data and complex geospatial queries; Elasticsearch can enable free-text and facet searches on metadata catalogs; MongoDB might store JSON metadata or product definitions; Timescale (built on Postgres) is tailored for time-indexed data – useful, for example, for satellite telemetry streams or logging who accessed what data when. The inclusion of these shows that modern ground segments are as much about IT and software engineering as about aerospace – handling big data is fundamentally a data management problem. Participants were likely shown how a user query like “find all Sentinel-2 images over Toulouse in July 2023 with <20% cloud” needs to hit a well-optimized catalog to return results in seconds.

* **Big Data Processing Frameworks:** To exploit large datasets, parallel computing frameworks are used. The training references **Dask** (a Python framework for parallel computing on large arrays and dataframes) and **CWL (Common Workflow Language)**. Dask allows one to write code that can scale from a laptop to a cluster by breaking the computation into tasks (it’s used to handle large xarray datasets, for instance, like opening huge stacks of satellite imagery that don’t fit in memory). CWL is a standard to define workflows and connect tools together in a reproducible way – likely introduced as a means to formalize complex processing pipelines (for example, a Level-2 atmospheric correction workflow that involves multiple steps and inputs could be codified in CWL and executed on the cluster). Embracing these frameworks is part of bringing **DevOps practices** into scientific data processing, which is necessary when dozens of developers and algorithms are involved in, say, a Sentinel ground segment that must run reliably.

Finally, the training underscored that the *value of data increases as it is processed and made accessible to users*. An adage mentioned was *“the value of data increases as it descends (from space to ground)”*, meaning raw data in orbit has potential, but its real utility is unlocked through processing, integration, and distribution. By investing in robust infrastructure and innovative data handling techniques, agencies ensure that terabytes of daily satellite data transform into actionable information for society – whether it’s a farmer using a moisture map, a city planner using heat maps, a researcher analyzing climate trends, or a machine learning model digesting petabytes of imagery to detect patterns we haven’t even thought of yet.

## Conclusion

The CNES “Data from Space” training provides a deep dive into the technical backbone of Earth observation and space science missions. From understanding how satellites in various orbits collect diverse data (optical images, radar scans, atmospheric profiles, altimetry) to exploring the ground systems that calibrate, process, and distribute this data, the course highlights the end-to-end complexity and ingenuity involved in turning raw space signals into meaningful insights. Key concepts like sensor types, orbit selection, and mission examples (Sentinel, SWOT, Gaia) set the stage, showing the *why and how* of data collection. The multitude of **applications** – climate monitoring, disaster management, urban planning, environmental enforcement, even planetary exploration – underscore the societal importance of these data.

Equally, the training’s focus on **data levels, quality, and processing techniques** demonstrates that raw data must pass through many stages (geometric correction, radiometric calibration, data fusion, etc.) to become analysis-ready. Participants gain appreciation for techniques like orthorectification and pan-sharpening that improve data utility, as well as advanced methods like InSAR or stereo-photogrammetry that extract new information (like ground deformation or 3D terrain) not directly evident in the raw data.

Lastly, the **infrastructure and big-data discussions** drive home that modern Earth observation is as much a software and IT challenge as it is an aerospace one. Handling petabytes from missions like SWOT or the Sentinel constellation requires cutting-edge HPC, clever data formats like Zarr, cloud integration, and a whole ecosystem of data hubs and services (Theia, ODATIS, etc.) to ensure users can find and exploit the data. The example performance improvement by adopting cloud-optimized formats and the scale of CNES’s TREX supercomputer illustrate how embracing technology is enabling the space community to keep up with the data deluge.

**Sources:** The information above is drawn from the CNES “Data from Space” training materials and corroborating references for definitions and context. Key references include NASA/ESA resources on remote sensing fundamentals (sensor types, orbits), Earth observation missions data (e.g., SWOT mission facts), standard data processing level definitions, and CNES’s published figures on its computing infrastructure. These sources ensure technical accuracy and up-to-date context for the topics covered.
